{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dynamic Workforce Shift & Exception Analytics - Demo Notebook\n",
        "\n",
        "This notebook demonstrates:\n",
        "- Synthetic data generation\n",
        "- ETL processing with PySpark\n",
        "- Exception detection\n",
        "- Anomaly detection\n",
        "- Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generate Synthetic Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data.synthetic_generator import SyntheticDataGenerator\n",
        "\n",
        "# Initialize generator\n",
        "generator = SyntheticDataGenerator(seed=42)\n",
        "\n",
        "# Generate employees\n",
        "employees = generator.generate_employees(count=50)\n",
        "print(f\"Generated {len(employees)} employees\")\n",
        "\n",
        "# Generate shifts\n",
        "shifts = generator.generate_shifts(employees)\n",
        "print(f\"Generated {len(shifts)} shifts\")\n",
        "\n",
        "# Generate attendance events\n",
        "start_date = datetime.now() - timedelta(days=14)\n",
        "events = generator.generate_attendance_events(\n",
        "    employees, shifts, start_date, days=14, rows=2000\n",
        ")\n",
        "print(f\"Generated {len(events)} attendance events\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "events_df = pd.DataFrame(events)\n",
        "events_df['event_timestamp'] = pd.to_datetime(events_df['event_timestamp'])\n",
        "\n",
        "print(\"\\nSample events:\")\n",
        "print(events_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Apply Exception Rules\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Visualize Raw Events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Event timeline\n",
        "fig = go.Figure()\n",
        "\n",
        "check_ins = events_df[events_df['event_type'] == 'CHECK_IN']\n",
        "check_outs = events_df[events_df['event_type'] == 'CHECK_OUT']\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=check_ins['event_timestamp'],\n",
        "    y=check_ins['employee_id'],\n",
        "    mode='markers',\n",
        "    name='Check-In',\n",
        "    marker=dict(symbol='triangle-up', size=8, color='green')\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=check_outs['event_timestamp'],\n",
        "    y=check_outs['employee_id'],\n",
        "    mode='markers',\n",
        "    name='Check-Out',\n",
        "    marker=dict(symbol='triangle-down', size=8, color='red')\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Attendance Events Timeline\",\n",
        "    xaxis_title=\"Time\",\n",
        "    yaxis_title=\"Employee ID\",\n",
        "    height=600\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run ETL Processing (Simplified)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For demo purposes, we'll simulate ETL processing\n",
        "# In production, this would use PySpark\n",
        "\n",
        "from src.rules.exception_engine import ExceptionEngine\n",
        "\n",
        "# Group events into sessions\n",
        "sessions = []\n",
        "session_id = 1\n",
        "\n",
        "for emp_id in events_df['employee_id'].unique()[:20]:  # Process first 20 employees\n",
        "    emp_events = events_df[events_df['employee_id'] == emp_id].sort_values('event_timestamp')\n",
        "    \n",
        "    i = 0\n",
        "    while i < len(emp_events):\n",
        "        if emp_events.iloc[i]['event_type'] == 'CHECK_IN':\n",
        "            check_in = emp_events.iloc[i]\n",
        "            \n",
        "            # Find corresponding check-out\n",
        "            check_out_idx = None\n",
        "            for j in range(i + 1, len(emp_events)):\n",
        "                if emp_events.iloc[j]['event_type'] == 'CHECK_OUT':\n",
        "                    check_out_idx = j\n",
        "                    break\n",
        "            \n",
        "            if check_out_idx:\n",
        "                check_out = emp_events.iloc[check_out_idx]\n",
        "                \n",
        "                # Calculate worked hours\n",
        "                worked_hours = (check_out['event_timestamp'] - check_in['event_timestamp']).total_seconds() / 3600\n",
        "                \n",
        "                session = {\n",
        "                    'session_id': session_id,\n",
        "                    'employee_id': emp_id,\n",
        "                    'actual_start': check_in['event_timestamp'],\n",
        "                    'actual_end': check_out['event_timestamp'],\n",
        "                    'worked_hours': worked_hours,\n",
        "                    'overtime_hours': max(0, worked_hours - 8),\n",
        "                    'is_partial': worked_hours < 6,\n",
        "                    'facility': check_in['facility']\n",
        "                }\n",
        "                \n",
        "                sessions.append(session)\n",
        "                session_id += 1\n",
        "                i = check_out_idx + 1\n",
        "            else:\n",
        "                i += 1\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "sessions_df = pd.DataFrame(sessions)\n",
        "print(f\"Computed {len(sessions_df)} work sessions\")\n",
        "print(\"\\nSample sessions:\")\n",
        "print(sessions_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exception_engine = ExceptionEngine()\n",
        "\n",
        "# Add shift times for demo (simplified)\n",
        "sessions_df['shift_start'] = sessions_df['actual_start'].dt.normalize() + pd.Timedelta(hours=9)\n",
        "sessions_df['shift_end'] = sessions_df['actual_start'].dt.normalize() + pd.Timedelta(hours=17)\n",
        "\n",
        "# Evaluate exceptions\n",
        "exception_codes_list = []\n",
        "exception_explanations_list = []\n",
        "\n",
        "for idx, row in sessions_df.iterrows():\n",
        "    session_dict = row.to_dict()\n",
        "    exceptions = exception_engine.evaluate_session(session_dict)\n",
        "    \n",
        "    codes = [e['code'] for e in exceptions]\n",
        "    explanations = {e['code']: e['explanation'] for e in exceptions}\n",
        "    \n",
        "    exception_codes_list.append(','.join(codes) if codes else None)\n",
        "    exception_explanations_list.append(explanations if explanations else None)\n",
        "\n",
        "sessions_df['exception_codes'] = exception_codes_list\n",
        "sessions_df['exception_explanations'] = exception_explanations_list\n",
        "\n",
        "# Count exceptions\n",
        "exceptions_with_data = sessions_df[sessions_df['exception_codes'].notna()]\n",
        "print(f\"Sessions with exceptions: {len(exceptions_with_data)}\")\n",
        "\n",
        "if len(exceptions_with_data) > 0:\n",
        "    print(\"\\nException examples:\")\n",
        "    for idx, row in exceptions_with_data.head(5).iterrows():\n",
        "        print(f\"\\nSession {row['session_id']} (Employee {row['employee_id']}):\")\n",
        "        print(f\"  Worked: {row['worked_hours']:.1f} hours\")\n",
        "        print(f\"  Exceptions: {row['exception_codes']}\")\n",
        "        if row['exception_explanations']:\n",
        "            for code, expl in row['exception_explanations'].items():\n",
        "                print(f\"    {code}: {expl}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Anomaly Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.models.anomaly_detector import AnomalyDetector\n",
        "\n",
        "# Initialize detector\n",
        "detector = AnomalyDetector(contamination=0.1, random_state=42)\n",
        "\n",
        "# Fit on sessions\n",
        "detector.fit(sessions_df)\n",
        "\n",
        "# Detect anomalies\n",
        "anomaly_scores, is_anomaly = detector.predict(sessions_df)\n",
        "\n",
        "sessions_df['anomaly_score'] = anomaly_scores\n",
        "sessions_df['is_anomaly'] = is_anomaly\n",
        "\n",
        "anomalies = sessions_df[sessions_df['is_anomaly']]\n",
        "print(f\"Detected {len(anomalies)} anomalous sessions ({len(anomalies)/len(sessions_df)*100:.1f}%)\")\n",
        "\n",
        "if len(anomalies) > 0:\n",
        "    print(\"\\nTop anomalous sessions:\")\n",
        "    top_anomalies = anomalies.nsmallest(5, 'anomaly_score')\n",
        "    for idx, row in top_anomalies.iterrows():\n",
        "        explanation = detector.explain_anomaly(row.to_dict(), row['anomaly_score'])\n",
        "        print(f\"\\nSession {row['session_id']} (Employee {row['employee_id']}):\")\n",
        "        print(f\"  Score: {row['anomaly_score']:.2f}\")\n",
        "        print(f\"  Explanation: {explanation['explanation']}\")\n",
        "        print(f\"  Top features: {[f['feature'] for f in explanation['top_features']]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Work hours distribution\n",
        "fig = px.histogram(\n",
        "    sessions_df,\n",
        "    x='worked_hours',\n",
        "    nbins=30,\n",
        "    labels={'worked_hours': 'Worked Hours', 'count': 'Frequency'},\n",
        "    title=\"Distribution of Worked Hours\"\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Workforce heatmap by hour\n",
        "sessions_df['hour'] = sessions_df['actual_start'].dt.hour\n",
        "\n",
        "if 'facility' in sessions_df.columns:\n",
        "    heatmap_data = sessions_df.groupby(['facility', 'hour']).size().reset_index(name='count')\n",
        "    \n",
        "    pivot_data = heatmap_data.pivot_table(\n",
        "        index='hour',\n",
        "        columns='facility',\n",
        "        values='count',\n",
        "        aggfunc='mean'\n",
        "    ).fillna(0)\n",
        "    \n",
        "    fig = px.imshow(\n",
        "        pivot_data.T,\n",
        "        labels=dict(x=\"Hour of Day\", y=\"Facility\", color=\"Employee Count\"),\n",
        "        x=[f\"{h:02d}:00\" for h in range(24)],\n",
        "        y=pivot_data.columns,\n",
        "        color_continuous_scale=\"YlOrRd\",\n",
        "        title=\"Average Workforce by Hour and Facility\"\n",
        "    )\n",
        "    fig.update_layout(height=400)\n",
        "    fig.show()\n",
        "else:\n",
        "    hourly_counts = sessions_df.groupby('hour').size()\n",
        "    fig = go.Figure(data=go.Bar(\n",
        "        x=[f\"{h:02d}:00\" for h in hourly_counts.index],\n",
        "        y=hourly_counts.values,\n",
        "        marker_color='steelblue'\n",
        "    ))\n",
        "    fig.update_layout(\n",
        "        title=\"Average Workforce by Hour\",\n",
        "        xaxis_title=\"Hour of Day\",\n",
        "        yaxis_title=\"Employee Count\",\n",
        "        height=400\n",
        "    )\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exception timeline\n",
        "if 'exception_codes' in sessions_df.columns:\n",
        "    exceptions_df = sessions_df[sessions_df['exception_codes'].notna()].copy()\n",
        "    \n",
        "    if not exceptions_df.empty:\n",
        "        exceptions_df['exception_list'] = exceptions_df['exception_codes'].str.split(',')\n",
        "        exceptions_df = exceptions_df.explode('exception_list')\n",
        "        exceptions_df['exception_list'] = exceptions_df['exception_list'].str.strip()\n",
        "        \n",
        "        exception_counts = exceptions_df['exception_list'].value_counts().reset_index()\n",
        "        exception_counts.columns = ['Exception Type', 'Count']\n",
        "        \n",
        "        fig = px.bar(\n",
        "            exception_counts,\n",
        "            x='Exception Type',\n",
        "            y='Count',\n",
        "            title=\"Exception Summary\"\n",
        "        )\n",
        "        fig.update_layout(xaxis_tickangle=-45)\n",
        "        fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anomaly score distribution\n",
        "fig = px.histogram(\n",
        "    sessions_df,\n",
        "    x='anomaly_score',\n",
        "    color='is_anomaly',\n",
        "    nbins=30,\n",
        "    labels={'anomaly_score': 'Anomaly Score', 'count': 'Frequency'},\n",
        "    title=\"Anomaly Score Distribution\"\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save summary statistics\n",
        "summary = {\n",
        "    'total_sessions': len(sessions_df),\n",
        "    'total_hours': sessions_df['worked_hours'].sum(),\n",
        "    'avg_hours': sessions_df['worked_hours'].mean(),\n",
        "    'sessions_with_exceptions': len(sessions_df[sessions_df['exception_codes'].notna()]),\n",
        "    'anomalous_sessions': len(sessions_df[sessions_df['is_anomaly']]),\n",
        "    'total_overtime': sessions_df['overtime_hours'].sum()\n",
        "}\n",
        "\n",
        "print(\"Summary Statistics:\")\n",
        "for key, value in summary.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Save heatmap as PNG (if kaleido is available)\n",
        "try:\n",
        "    if 'facility' in sessions_df.columns:\n",
        "        heatmap_data = sessions_df.groupby(['facility', 'hour']).size().reset_index(name='count')\n",
        "        pivot_data = heatmap_data.pivot_table(\n",
        "            index='hour',\n",
        "            columns='facility',\n",
        "            values='count',\n",
        "            aggfunc='mean'\n",
        "        ).fillna(0)\n",
        "        \n",
        "        fig = px.imshow(\n",
        "            pivot_data.T,\n",
        "            labels=dict(x=\"Hour of Day\", y=\"Facility\", color=\"Employee Count\"),\n",
        "            x=[f\"{h:02d}:00\" for h in range(24)],\n",
        "            y=pivot_data.columns,\n",
        "            color_continuous_scale=\"YlOrRd\",\n",
        "            title=\"Average Workforce by Hour and Facility\"\n",
        "        )\n",
        "        fig.update_layout(height=600, width=1000)\n",
        "        \n",
        "        # Save to output directory\n",
        "        output_path = '../output/demo_summary.png'\n",
        "        fig.write_image(output_path)\n",
        "        print(f\"\\nSaved heatmap to {output_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nCould not save PNG (kaleido may not be installed): {e}\")\n",
        "    print(\"Install with: pip install kaleido\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
